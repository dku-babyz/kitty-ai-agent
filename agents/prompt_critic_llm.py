# agents/prompt_critic_llm.py
import openai
import os
import json
import re

class PromptCriticLLM:
    def __init__(
        self,
        model="gpt-4o",
        rule_path="rules/prompt_critic_rule.txt",
        story_prompt_path="prompts/story_prompt.txt",
        image_prompt_path="prompts/image_prompt.txt",
        story_feedback_path="memory/story_feedback.txt",
        image_feedback_path="memory/image_feedback.txt",
        output_path="results/revised_story_prompt.txt"
    ):
        self.model = model
        self.rule_path = rule_path
        self.story_prompt_path = story_prompt_path
        self.image_prompt_path = image_prompt_path
        self.story_feedback_path = story_feedback_path
        self.image_feedback_path = image_feedback_path
        self.output_path = output_path

        self.rule = self._load_file(rule_path)
        self.story_prompt = self._load_file(story_prompt_path)
        self.image_prompt = self._load_file(image_prompt_path)
        self.story_feedback = self._load_file(story_feedback_path)
        self.image_feedback = self._load_file(image_feedback_path)

    def _load_file(self, path):
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        with open(path, "r", encoding="utf-8") as f:
            return f.read()

    def _extract_json_from_text(self, text):
        match = re.search(r"\{(?:.|\n)*\}", text)
        return match.group(0) if match else text

    def build_prompt(self):
        return self.rule \
            .replace("{STORY_PROMPT}", self.story_prompt.strip()) \
            .replace("{IMAGE_PROMPT}", self.image_prompt.strip()) \
            .replace("{STORY_FEEDBACK}", self.story_feedback.strip()) \
            .replace("{IMAGE_FEEDBACK}", self.image_feedback.strip())

    def generate(self):
        full_prompt = self.build_prompt()
        print("\nğŸ“„ === ì „ë‹¬ëœ í‰ê°€ í”„ë¡¬í”„íŠ¸ ===")
        print(full_prompt)

        client = openai.OpenAI()
        response = client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” í”„ë¡¬í”„íŠ¸ í‰ê°€ìì´ì í¸ì§‘ìì•¼. RULEì„ ë”°ë¥´ê³ , JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ì™€ ë¦¬ë¹„ì „ì„ ë°˜í™˜í•´."},
                {"role": "user", "content": full_prompt}
            ],
            temperature=0.5,
            max_tokens=1200
        )

        content = response.choices[0].message.content.strip()
        content_cleaned = self._extract_json_from_text(content)

        try:
            result = json.loads(content_cleaned)
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON format", "raw": content}

        with open(self.output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… í”„ë¡¬í”„íŠ¸ ë¦¬ë¹„ì „ ì™„ë£Œ ë° ì €ì¥ë¨: {self.output_path}")
        return result


# âœ… ì™¸ë¶€ ëª¨ë“ˆì—ì„œ ì‰½ê²Œ ì“¸ ìˆ˜ ìˆê²Œ call() í•¨ìˆ˜ ì œê³µ
def call(
    model="gpt-4o",
    rule_path="rules/prompt_critic_rule.txt",
    story_prompt_path="prompts/story_prompt.txt",
    image_prompt_path="prompts/image_prompt.txt",
    story_feedback_path="memory/story_feedback.txt",
    image_feedback_path="memory/image_feedback.txt",
    output_path="results/revised_story_prompt.txt"
):
    critic = PromptCriticLLM(
        model=model,
        rule_path=rule_path,
        story_prompt_path=story_prompt_path,
        image_prompt_path=image_prompt_path,
        story_feedback_path=story_feedback_path,
        image_feedback_path=image_feedback_path,
        output_path=output_path
    )
    return critic.generate()


# ğŸ§ª ë‹¨ë… ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    print("ğŸ§  Prompt Critic LLM ì‹¤í–‰ ì¤‘...")
    result = call()
    print("\nğŸ“¤ í‰ê°€ ê²°ê³¼:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
